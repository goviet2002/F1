{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import certifi\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "head = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "base_url = \"https://www.formula1.com\"\n",
    "\n",
    "# Set up Session for requests\n",
    "session = requests.Session()\n",
    "\n",
    "# Get years that statistics have been published\n",
    "current_year = datetime.now().year\n",
    "years = [year for year in range(1950, current_year + 1)]\n",
    "\n",
    "# Helper function to save data to CSV\n",
    "def save_to_json(data, headers, filename):\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_json(filename, orient='records', lines=True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_races_year(year):\n",
    "    # Default values in case elements are not found\n",
    "    race_date = None\n",
    "    circuit = None\n",
    "    city = None\n",
    "    \n",
    "    # URL of the page\n",
    "    url = f\"{base_url}/en/results/{year}/races\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = session.get(url, headers=head, verify=certifi.where())\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        \n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            \n",
    "            rows = table.find('tbody').find_all('tr')\n",
    "            data = []\n",
    "            race_links = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                row_data = []\n",
    "                \n",
    "                for i, col in enumerate(cols):\n",
    "                    if i == 2: #Driver column\n",
    "                        winner = col.text.strip().replace(\"\\xa0\", \" \")[:-3]\n",
    "                        row_data.append(winner)\n",
    "                    else:\n",
    "                        row_data.append(col.text.strip())\n",
    "                        \n",
    "                # Append the row data to the data list (only once per row)\n",
    "                data.append(row_data)\n",
    "                \n",
    "                # Extract race link\n",
    "                race_link = cols[0].find('a')['href']\n",
    "                full_link = f\"{base_url}/en/results/{year}/{race_link}\"\n",
    "                race_links.append((row_data[0], full_link))\n",
    "                \n",
    "    return data, headers, race_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_location(race_url):\n",
    "    response = session.get(race_url, headers=head, verify=certifi.where())\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the location table\n",
    "        header_section = soup.find('div', class_='max-tablet:flex-col flex gap-xs')\n",
    "        \n",
    "        if header_section:\n",
    "            location_info = header_section.find_all('p')\n",
    "            \n",
    "            race_date = location_info[0].text.strip()[:-5]\n",
    "            track = location_info[1].text.strip().split(\", \")\n",
    "            circuit = track[0]\n",
    "            city = track[1]\n",
    "            \n",
    "    return race_date, circuit, city\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afded3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_results(race_link, type):\n",
    "    base_url = race_link.rsplit('/', 1)[0]\n",
    "    response = session.get(f'{base_url}/{type}', headers=head, verify=certifi.where())\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the location table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        \n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            \n",
    "            rows = table.find('tbody').find_all('tr')\n",
    "            data = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                row_data = []\n",
    "                \n",
    "                for i, col in enumerate(cols):\n",
    "                    if i == 2: #Driver column\n",
    "                        winner = col.text.strip().replace(\"\\xa0\", \" \")[:-3]\n",
    "                        row_data.append(winner)\n",
    "                    else:\n",
    "                        row_data.append(col.text.strip())\n",
    "                        \n",
    "                data.append(row_data)\n",
    "    return headers, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85e57fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_race = []\n",
    "races = []\n",
    "\n",
    "headers_race_location = ['Grand Prix', 'Circuit', 'Country/City', 'Year', 'Date']\n",
    "race_location = []\n",
    "\n",
    "race_result_type = ['race-result', 'fastest-laps', 'pit-stop-summary', 'starting-grid', 'qualifying',\n",
    "                'practice/3', 'practice/2', 'practice/1']\n",
    "\n",
    "# Collect all race links\n",
    "all_race_links = []\n",
    "for year in years:\n",
    "    race, header_race, race_links = scrape_races_year(year)\n",
    "    races.extend(race)\n",
    "    \n",
    "    all_race_links.extend([(link[0], link[1]) for link in race_links])\n",
    "\n",
    "    if len(headers_race) == 0:\n",
    "        headers_race = header_race\n",
    "        \n",
    "# Create a list of all tasks for race results\n",
    "all_result_tasks = []\n",
    "for race_link in all_race_links:\n",
    "    for result_type in race_result_type:\n",
    "        all_result_tasks.append((race_link[1], result_type))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e2b5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process functions in parallel to get data\n",
    "def process_race_location(race_link_tuple):\n",
    "    grand_prix, url = race_link_tuple\n",
    "    year = url.split('/results/')[1].split('/')[0]\n",
    "\n",
    "    try:\n",
    "        race_date, circuit, city = scrape_race_location(url)\n",
    "        return [grand_prix, circuit, city, year, race_date]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def process_race_results(args):\n",
    "    race_link_tuple, result_type = args\n",
    "    grand_prix, url = race_link_tuple\n",
    "    year = url.split('/results/')[1].split('/')[0]\n",
    "    \n",
    "    try:\n",
    "        header_result, data_result = scrape_race_results(url, result_type)\n",
    "        \n",
    "        if header_result and data_result:\n",
    "            new_header = [\"Grand Prix\", \"Year\"] + header_result\n",
    "            new_data = [[grand_prix, year] + row for row in data_result]\n",
    "            \n",
    "            file_name = result_type.replace(\"/\", \"-\").replace(\"-\", \"_\")\n",
    "            \n",
    "            return new_header, new_data, file_name\n",
    "        else:\n",
    "            print(f\"No data found for {url}, {result_type}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}, {result_type}: {e}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ThreadPoolExecutor to manage threads\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=25) as executor:\n",
    "    # Submit location tasks\n",
    "    location_futures = {executor.submit(process_race_location, link): link for link in all_race_links}\n",
    "    \n",
    "    # Submit result tasks\n",
    "    result_futures = {executor.submit(scrape_race_results, task[0], task[1]): task for task in all_result_tasks}\n",
    "      \n",
    "    # Process location results\n",
    "    race_location = []\n",
    "    for future in concurrent.futures.as_completed(location_futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            race_location.append(result) \n",
    "    \n",
    "    # Process race results\n",
    "    race_result = {}\n",
    "    for future in concurrent.futures.as_completed(result_futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            race_result[result[2]] = {\n",
    "                \"header\": result[0],\n",
    "                \"data\": result[1]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b508b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'race_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result_type, result_data \u001b[38;5;129;01min\u001b[39;00m race_result\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      2\u001b[0m     save_to_json(result_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m], result_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m], result_type)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save race and location data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'race_result' is not defined"
     ]
    }
   ],
   "source": [
    "# Save to JSON\n",
    "for result_type, result_data in race_result.items():\n",
    "    save_to_json(result_data['data'], result_data['header'], result_type)\n",
    "    \n",
    "# Save race and location data\n",
    "save_to_json(races, headers_race, \"races\")\n",
    "save_to_json(race_location, headers_race_location, \"race_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8776c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Pos', 'No', 'Driver', 'Car', 'Q1', 'Q2', 'Q3', 'Laps'], [['1', '4', 'Lando Norris', 'McLaren Mercedes', '1:15.912', '1:15.415', '1:15.096', '20'], ['2', '81', 'Oscar Piastri', 'McLaren Mercedes', '1:16.062', '1:15.468', '1:15.180', '18'], ['3', '1', 'Max Verstappen', 'Red Bull Racing Honda RBPT', '1:16.018', '1:15.565', '1:15.481', '17'], ['4', '63', 'George Russell', 'Mercedes', '1:15.971', '1:15.798', '1:15.546', '21'], ['5', '22', 'Yuki Tsunoda', 'Racing Bulls Honda RBPT', '1:16.225', '1:16.009', '1:15.670', '18'], ['6', '23', 'Alexander Albon', 'Williams Mercedes', '1:16.245', '1:16.017', '1:15.737', '21'], ['7', '16', 'Charles Leclerc', 'Ferrari', '1:16.029', '1:15.827', '1:15.755', '20'], ['8', '44', 'Lewis Hamilton', 'Ferrari', '1:16.213', '1:15.919', '1:15.973', '23'], ['9', '10', 'Pierre Gasly', 'Alpine Renault', '1:16.328', '1:16.112', '1:15.980', '21'], ['10', '55', 'Carlos Sainz', 'Williams Mercedes', '1:16.360', '1:15.931', '1:16.062', '21'], ['11', '6', 'Isack Hadjar', 'Racing Bulls Honda RBPT', '1:16.354', '1:16.175', '', '12'], ['12', '14', 'Fernando Alonso', 'Aston Martin Aramco Mercedes', '1:16.288', '1:16.453', '', '13'], ['13', '18', 'Lance Stroll', 'Aston Martin Aramco Mercedes', '1:16.369', '1:16.483', '', '15'], ['14', '7', 'Jack Doohan', 'Alpine Renault', '1:16.315', '1:16.863', '', '15'], ['15', '5', 'Gabriel Bortoleto', 'Kick Sauber Ferrari', '1:16.516', '1:17.520', '', '13'], ['16', '12', 'Kimi Antonelli', 'Mercedes', '1:16.525', '', '', '9'], ['17', '27', 'Nico Hulkenberg', 'Kick Sauber Ferrari', '1:16.579', '', '', '9'], ['18', '30', 'Liam Lawson', 'Red Bull Racing Honda RBPT', '1:17.094', '', '', '7'], ['19', '31', 'Esteban Ocon', 'Haas Ferrari', '1:17.147', '', '', '9'], ['NC', '87', 'Oliver Bearman', 'Haas Ferrari', 'DNS', '', '', '1']])\n"
     ]
    }
   ],
   "source": [
    "print(scrape_race_results(\"https://www.formula1.com/en/results/2025/races/1254/australia/race-result\", \"qualifying\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
