{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anhvi\\OneDrive\\Desktop\\F1 Projekt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "from utils.path import get_project_root\n",
    "from utils.f1_shared import ssl_context, head, base_url, years, test_function\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"f1_drivers_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "CHECKPOINTS_DIR = os.path.join(PROJECT_ROOT, \"data\", \"f1_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_drivers_standing(session, year):\n",
    "    \"\"\"Scrape driver standings for a specific year\"\"\"\n",
    "    url = f\"{base_url}/en/results/{year}/drivers\"\n",
    "    \n",
    "    async with session.get(url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {url}. Status: {response.status}\")\n",
    "            return [], [], []\n",
    "\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Find table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        \n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            # Add YEAR to headers\n",
    "            if 'Year' not in headers:\n",
    "                headers.append('Year')\n",
    "            \n",
    "            rows = table.find('tbody').find_all('tr')\n",
    "            data = []\n",
    "            driver_links = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                row_data = []\n",
    "                \n",
    "                for i, col in enumerate(cols):\n",
    "                    if i == 1:  # Driver name column\n",
    "                        # Extract first and last name separately, ignoring the driver code\n",
    "                        driver_a = col.find('a')\n",
    "                        if driver_a:\n",
    "                            first_name_span = driver_a.find('span', class_='max-desktop:hidden')\n",
    "                            last_name_span = driver_a.find('span', class_='max-tablet:hidden')\n",
    "                            \n",
    "                            first_name = first_name_span.text.strip() if first_name_span else \"\"\n",
    "                            last_name = last_name_span.text.strip() if last_name_span else \"\"\n",
    "                            \n",
    "                            full_name = f\"{first_name} {last_name}\".strip()\n",
    "                            row_data.append(full_name)\n",
    "                        else:\n",
    "                            row_data.append(col.text.strip())\n",
    "                    else:\n",
    "                        row_data.append(col.text.strip())\n",
    "                \n",
    "                # Add year to each row\n",
    "                row_data.append(str(year))\n",
    "                data.append(row_data)\n",
    "                \n",
    "                # Extract driver link\n",
    "                driver_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "                if driver_link:\n",
    "                    # Make sure the driver_link starts with a slash if needed\n",
    "                    if not driver_link.startswith('/'):\n",
    "                        driver_link = f\"/{driver_link}\"\n",
    "                        \n",
    "                    # Add the full URL\n",
    "                    full_link = f\"{base_url}/en/results/{year}{driver_link}\"\n",
    "                    driver_links.append((row_data[1], full_link, year))  # Add year to link tuple\n",
    "                \n",
    "        return data, headers, driver_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Max Verstappen', 'https://www.formula1.com/en/results/2024/drivers/MAXVER01/max-verstappen', 2024), ('Lando Norris', 'https://www.formula1.com/en/results/2024/drivers/LANNOR01/lando-norris', 2024), ('Charles Leclerc', 'https://www.formula1.com/en/results/2024/drivers/CHALEC01/charles-leclerc', 2024), ('Oscar Piastri', 'https://www.formula1.com/en/results/2024/drivers/OSCPIA01/oscar-piastri', 2024), ('Carlos Sainz', 'https://www.formula1.com/en/results/2024/drivers/CARSAI01/carlos-sainz', 2024), ('George Russell', 'https://www.formula1.com/en/results/2024/drivers/GEORUS01/george-russell', 2024), ('Lewis Hamilton', 'https://www.formula1.com/en/results/2024/drivers/LEWHAM01/lewis-hamilton', 2024), ('Sergio Perez', 'https://www.formula1.com/en/results/2024/drivers/SERPER01/sergio-perez', 2024), ('Fernando Alonso', 'https://www.formula1.com/en/results/2024/drivers/FERALO01/fernando-alonso', 2024), ('Pierre Gasly', 'https://www.formula1.com/en/results/2024/drivers/PIEGAS01/pierre-gasly', 2024), ('Nico Hulkenberg', 'https://www.formula1.com/en/results/2024/drivers/NICHUL01/nico-hulkenberg', 2024), ('Yuki Tsunoda', 'https://www.formula1.com/en/results/2024/drivers/YUKTSU01/yuki-tsunoda', 2024), ('Lance Stroll', 'https://www.formula1.com/en/results/2024/drivers/LANSTR01/lance-stroll', 2024), ('Esteban Ocon', 'https://www.formula1.com/en/results/2024/drivers/ESTOCO01/esteban-ocon', 2024), ('Kevin Magnussen', 'https://www.formula1.com/en/results/2024/drivers/KEVMAG01/kevin-magnussen', 2024), ('Alexander Albon', 'https://www.formula1.com/en/results/2024/drivers/ALEALB01/alexander-albon', 2024), ('Daniel Ricciardo', 'https://www.formula1.com/en/results/2024/drivers/DANRIC01/daniel-ricciardo', 2024), ('Oliver Bearman', 'https://www.formula1.com/en/results/2024/drivers/OLIBEA01/oliver-bearman', 2024), ('Franco Colapinto', 'https://www.formula1.com/en/results/2024/drivers/FRACOL01/franco-colapinto', 2024), ('Guanyu Zhou', 'https://www.formula1.com/en/results/2024/drivers/GUAZHO01/guanyu-zhou', 2024), ('Liam Lawson', 'https://www.formula1.com/en/results/2024/drivers/LIALAW01/liam-lawson', 2024), ('Valtteri Bottas', 'https://www.formula1.com/en/results/2024/drivers/VALBOT01/valtteri-bottas', 2024), ('Logan Sargeant', 'https://www.formula1.com/en/results/2024/drivers/LOGSAR01/logan-sargeant', 2024), ('Jack Doohan', 'https://www.formula1.com/en/results/2024/drivers/JACDOO01/jack-doohan', 2024)]\n",
      "   Pos            Driver Nationality                           Car  Pts  Year\n",
      "0    1    Max Verstappen         NED    Red Bull Racing Honda RBPT  437  2024\n",
      "1    2      Lando Norris         GBR              McLaren Mercedes  374  2024\n",
      "2    3   Charles Leclerc         MON                       Ferrari  356  2024\n",
      "3    4     Oscar Piastri         AUS              McLaren Mercedes  292  2024\n",
      "4    5      Carlos Sainz         ESP                       Ferrari  290  2024\n",
      "5    6    George Russell         GBR                      Mercedes  245  2024\n",
      "6    7    Lewis Hamilton         GBR                      Mercedes  223  2024\n",
      "7    8      Sergio Perez         MEX    Red Bull Racing Honda RBPT  152  2024\n",
      "8    9   Fernando Alonso         ESP  Aston Martin Aramco Mercedes   70  2024\n",
      "9   10      Pierre Gasly         FRA                Alpine Renault   42  2024\n",
      "10  11   Nico Hulkenberg         GER                  Haas Ferrari   41  2024\n",
      "11  12      Yuki Tsunoda         JPN                 RB Honda RBPT   30  2024\n",
      "12  13      Lance Stroll         CAN  Aston Martin Aramco Mercedes   24  2024\n",
      "13  14      Esteban Ocon         FRA                Alpine Renault   23  2024\n",
      "14  15   Kevin Magnussen         DEN                  Haas Ferrari   16  2024\n",
      "15  16   Alexander Albon         THA             Williams Mercedes   12  2024\n",
      "16  17  Daniel Ricciardo         AUS                 RB Honda RBPT   12  2024\n",
      "17  18    Oliver Bearman         GBR                  Haas Ferrari    7  2024\n",
      "18  19  Franco Colapinto         ARG             Williams Mercedes    5  2024\n",
      "19  20       Guanyu Zhou         CHN           Kick Sauber Ferrari    4  2024\n",
      "20  21       Liam Lawson         NZL                 RB Honda RBPT    4  2024\n",
      "21  22   Valtteri Bottas         FIN           Kick Sauber Ferrari    0  2024\n",
      "22  23    Logan Sargeant         USA             Williams Mercedes    0  2024\n",
      "23  24       Jack Doohan         AUS                Alpine Renault    0  2024\n"
     ]
    }
   ],
   "source": [
    "result = await test_function(2024, scrape_drivers_standing)\n",
    "df = pd.DataFrame(result[0], columns=result[1])\n",
    "print(result[2])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_driver_results(session, driver_url):\n",
    "    \"\"\"Scrape detailed information for a specific driver\"\"\"\n",
    "    async with session.get(driver_url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {driver_url}. Status: {response.status}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Extract driver code from URL\n",
    "        url_parts = driver_url.split('/')\n",
    "        driver_code = url_parts[-2] if len(url_parts) > 2 else None\n",
    "        \n",
    "        # Get the race results table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        if not table:\n",
    "            print(f\"No results table found for {driver_url}\")\n",
    "            return [], [], driver_code\n",
    "            \n",
    "        # Get headers\n",
    "        headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "        \n",
    "        # Get race results\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            row_data = []\n",
    "            \n",
    "            for col in cols:\n",
    "                row_data.append(col.text.strip())\n",
    "                \n",
    "            data.append(row_data)\n",
    "                \n",
    "        return data, headers, driver_code\n",
    "    \n",
    "async def process_driver_data(session, driver_link_tuple):\n",
    "    \"\"\"Process a driver link to get detailed information\"\"\"\n",
    "    driver_name, url = driver_link_tuple\n",
    "    \n",
    "    try:\n",
    "        data, headers, driver_code = await scrape_driver_results(session, url)\n",
    "        \n",
    "        # Create a driver details dictionary with all the data\n",
    "        driver_details = {\n",
    "            'name': driver_name,\n",
    "            'driver_code': driver_code,\n",
    "            'url': url,\n",
    "            'headers': headers,\n",
    "            'race_results': data\n",
    "        }\n",
    "        \n",
    "        return driver_details\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing driver {driver_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANNOR01\n",
      "       Grand prix         Date              Car Race position Pts\n",
      "0       Australia  17 Mar 2019  McLaren Renault            12   0\n",
      "1         Bahrain  31 Mar 2019  McLaren Renault             6   8\n",
      "2           China  14 Apr 2019  McLaren Renault           DNF   0\n",
      "3      Azerbaijan  28 Apr 2019  McLaren Renault             8   4\n",
      "4           Spain  12 May 2019  McLaren Renault           DNF   0\n",
      "5          Monaco  26 May 2019  McLaren Renault            11   0\n",
      "6          Canada  09 Jun 2019  McLaren Renault           DNF   0\n",
      "7          France  23 Jun 2019  McLaren Renault             9   2\n",
      "8         Austria  30 Jun 2019  McLaren Renault             6   8\n",
      "9   Great Britain  14 Jul 2019  McLaren Renault            11   0\n",
      "10        Germany  28 Jul 2019  McLaren Renault           DNF   0\n",
      "11        Hungary  04 Aug 2019  McLaren Renault             9   2\n",
      "12        Belgium  01 Sep 2019  McLaren Renault           DNF   0\n",
      "13          Italy  08 Sep 2019  McLaren Renault            10   1\n",
      "14      Singapore  22 Sep 2019  McLaren Renault             7   6\n",
      "15         Russia  29 Sep 2019  McLaren Renault             8   4\n",
      "16          Japan  13 Oct 2019  McLaren Renault            11   0\n",
      "17         Mexico  27 Oct 2019  McLaren Renault           DNF   0\n",
      "18  United States  03 Nov 2019  McLaren Renault             7   6\n",
      "19         Brazil  17 Nov 2019  McLaren Renault             8   4\n",
      "20      Abu Dhabi  01 Dec 2019  McLaren Renault             8   4\n",
      "{'name': 'Lando Norris', 'driver_code': 'LANNOR01', 'url': 'https://www.formula1.com/en/results/2019/drivers/LANNOR01/lando-norris', 'headers': ['Grand prix', 'Date', 'Car', 'Race position', 'Pts'], 'race_results': [['Australia', '17 Mar 2019', 'McLaren Renault', '12', '0'], ['Bahrain', '31 Mar 2019', 'McLaren Renault', '6', '8'], ['China', '14 Apr 2019', 'McLaren Renault', 'DNF', '0'], ['Azerbaijan', '28 Apr 2019', 'McLaren Renault', '8', '4'], ['Spain', '12 May 2019', 'McLaren Renault', 'DNF', '0'], ['Monaco', '26 May 2019', 'McLaren Renault', '11', '0'], ['Canada', '09 Jun 2019', 'McLaren Renault', 'DNF', '0'], ['France', '23 Jun 2019', 'McLaren Renault', '9', '2'], ['Austria', '30 Jun 2019', 'McLaren Renault', '6', '8'], ['Great Britain', '14 Jul 2019', 'McLaren Renault', '11', '0'], ['Germany', '28 Jul 2019', 'McLaren Renault', 'DNF', '0'], ['Hungary', '04 Aug 2019', 'McLaren Renault', '9', '2'], ['Belgium', '01 Sep 2019', 'McLaren Renault', 'DNF', '0'], ['Italy', '08 Sep 2019', 'McLaren Renault', '10', '1'], ['Singapore', '22 Sep 2019', 'McLaren Renault', '7', '6'], ['Russia', '29 Sep 2019', 'McLaren Renault', '8', '4'], ['Japan', '13 Oct 2019', 'McLaren Renault', '11', '0'], ['Mexico', '27 Oct 2019', 'McLaren Renault', 'DNF', '0'], ['United States', '03 Nov 2019', 'McLaren Renault', '7', '6'], ['Brazil', '17 Nov 2019', 'McLaren Renault', '8', '4'], ['Abu Dhabi', '01 Dec 2019', 'McLaren Renault', '8', '4']]}\n"
     ]
    }
   ],
   "source": [
    "result = await test_function('https://www.formula1.com/en/results/2019/drivers/LANNOR01/lando-norris', scrape_driver_results)\n",
    "df = pd.DataFrame(result[0], columns=result[1])\n",
    "print(result[2])\n",
    "print(df)\n",
    "tuple= (\"Lando Norris\", \"https://www.formula1.com/en/results/2019/drivers/LANNOR01/lando-norris\")\n",
    "result = await test_function(tuple, process_driver_data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_driver_links():\n",
    "    \"\"\"Collect all driver links across years\"\"\"\n",
    "    all_driver_links = []\n",
    "    headers_drivers = []\n",
    "    drivers = []\n",
    "\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector) as session:          \n",
    "        tasks = [scrape_drivers_standing(session, year) for year in years]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "        for driver_data, header_driver, driver_links in results:\n",
    "            drivers.extend(driver_data)\n",
    "            all_driver_links.extend([(link[0], link[1], link[2]) for link in driver_links])\n",
    "\n",
    "            if len(headers_drivers) == 0:\n",
    "                headers_drivers = header_driver\n",
    "                \n",
    "        # Save the drivers data to a JSON file (renamed to race_standing.json)\n",
    "        drivers_data = {\n",
    "            \"headers\": headers_drivers,\n",
    "            \"drivers\": drivers\n",
    "        }    \n",
    "        with open(os.path.join(DATA_DIR, \"race_standing.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(drivers_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved {len(drivers)} driver standings to race_standing.json\")\n",
    "                \n",
    "        return all_driver_links, headers_drivers, drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_driver_profile(session, driver_name, driver_code):\n",
    "    \"\"\"Scrape detailed profile information for a driver from the main drivers page\"\"\"\n",
    "    # Extract the last name part from the driver code URL\n",
    "    name_part = driver_name.lower().replace(' ', '-')\n",
    "    \n",
    "    # Construct profile URL\n",
    "    profile_url = f\"{base_url}/en/drivers/{name_part}.html\"\n",
    "    \n",
    "    try:\n",
    "        async with session.get(profile_url, headers=head) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Driver profile not found: {profile_url}. Status: {response.status}\")\n",
    "                return None\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "            # Get the driver info section (with Team, Country, etc.)\n",
    "            driver_info_section = soup.find('div', class_='f1-dl')\n",
    "            \n",
    "            # Initialize lists for headers and data\n",
    "            headers = [\"name\", \"driver_code\", \"profile_url\"]\n",
    "            data = [driver_name, driver_code, profile_url]\n",
    "            \n",
    "            if driver_info_section:\n",
    "                # Extract info from dt/dd pairs\n",
    "                dt_elements = driver_info_section.find_all('dt')\n",
    "                dd_elements = driver_info_section.find_all('dd')\n",
    "                \n",
    "                for dt, dd in zip(dt_elements, dd_elements):\n",
    "                    # Convert header to lowercase with underscores\n",
    "                    header = dt.text.strip().lower().replace(' ', '_')\n",
    "                    headers.append(header)\n",
    "                    data.append(dd.text.strip())\n",
    "            \n",
    "            # Get biographical info (DOB, birthplace)\n",
    "            bio_section = soup.find('div', class_='biography')\n",
    "            if bio_section:\n",
    "                bio_items = bio_section.find_all('p')\n",
    "                for item in bio_items:\n",
    "                    text = item.text.strip()\n",
    "                    if text.startswith(\"Date of birth\"):\n",
    "                        headers.append(\"date_of_birth\")\n",
    "                        data.append(text.replace(\"Date of birth\", \"\").strip())\n",
    "                    elif text.startswith(\"Place of birth\"):\n",
    "                        headers.append(\"place_of_birth\")\n",
    "                        data.append(text.replace(\"Place of birth\", \"\").strip())\n",
    "            \n",
    "            # Get driver image - try multiple approaches\n",
    "            driver_img = None\n",
    "\n",
    "            # Try main profile image first\n",
    "            for img in soup.find_all('img', class_='f1-c-image'):\n",
    "                if img.get('alt') and driver_name.lower() in img.get('alt').lower():\n",
    "                    driver_img = img\n",
    "                    break\n",
    "\n",
    "            # Alternative approach using figure element\n",
    "            if not driver_img:\n",
    "                driver_figure = soup.find('figure', class_='f1-utils-flex-container')\n",
    "                if driver_figure:\n",
    "                    driver_img = driver_figure.find('img')\n",
    "\n",
    "            # Extract image URL from the first approach that worked\n",
    "            if driver_img:\n",
    "                img_url = None\n",
    "                for attr in ['src', 'data-src', 'srcset']:\n",
    "                    if attr in driver_img.attrs:\n",
    "                        img_url = driver_img[attr]\n",
    "                        break\n",
    "                        \n",
    "                if img_url:\n",
    "                    headers.append(\"image_url\")\n",
    "                    data.append(img_url)\n",
    "            \n",
    "            #Find helmet image\n",
    "            helmet_img = None\n",
    "            helmet_figure = soup.find('figure', class_='f1-driver-helmet')\n",
    "            if helmet_figure:\n",
    "                helmet_img = helmet_figure.find('img')\n",
    "                if helmet_img:\n",
    "                    helmet_url = None\n",
    "                    for attr in ['src', 'data-src', 'srcset']:\n",
    "                        if attr in helmet_img.attrs:\n",
    "                            helmet_url = helmet_img[attr]\n",
    "                            break\n",
    "                    \n",
    "                    if helmet_url:\n",
    "                        headers.append(\"helmet_url\")\n",
    "                        data.append(helmet_url)\n",
    "                        \n",
    "            # If no helmet image found in HTML, construct URL based on F1 pattern\n",
    "            if \"helmet_url\" not in headers:\n",
    "                # Get last name from driver name\n",
    "                last_name = driver_name.split()[-1].lower()\n",
    "                current_year = 2025  # Update as needed\n",
    "                \n",
    "                # Construct helmet URL following F1's pattern\n",
    "                constructed_helmet_url = f\"https://media.formula1.com/image/upload/f_auto,c_limit,q_auto,w_1024/fom-website/manual/Helmets{current_year}/{last_name}\"\n",
    "                \n",
    "                headers.append(\"helmet_url\")\n",
    "                data.append(constructed_helmet_url)\n",
    "            \n",
    "            return headers, data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping profile for {driver_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "async def collect_current_driver_profiles(current_year=years[-1]):\n",
    "    \"\"\"Collect detailed profiles for current season drivers\"\"\"\n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Get current season drivers\n",
    "        current_drivers_data = await scrape_drivers_standing(session, current_year)\n",
    "        drivers_data, headers, driver_links = current_drivers_data\n",
    "                \n",
    "        # Process each driver profile\n",
    "        all_headers = []\n",
    "        driver_profiles = []\n",
    "        \n",
    "        for driver_name, driver_url, year in driver_links:\n",
    "            # Extract driver code from URL\n",
    "            url_parts = driver_url.split('/')\n",
    "            driver_code = url_parts[-2] if len(url_parts) > 2 else None\n",
    "            \n",
    "            if driver_code:\n",
    "                headers, data = await scrape_driver_profile(session, driver_name, driver_code)\n",
    "                if headers and data:\n",
    "                    # Update all_headers to include any new fields\n",
    "                    for header in headers:\n",
    "                        if header not in all_headers:\n",
    "                            all_headers.append(header)\n",
    "                    \n",
    "                    driver_profiles.append(data)\n",
    "        \n",
    "        # Normalize data - ensure all rows have the same number of fields\n",
    "        normalized_profiles = []\n",
    "        for profile in driver_profiles:\n",
    "            # Create a dict from the headers and data\n",
    "            profile_dict = dict(zip(headers, profile))\n",
    "            \n",
    "            # Create a new row with all headers\n",
    "            normalized_row = []\n",
    "            for header in all_headers:\n",
    "                normalized_row.append(profile_dict.get(header, \"\"))\n",
    "            \n",
    "            normalized_profiles.append(normalized_row)\n",
    "        \n",
    "        # Save profiles to a JSON file in table format\n",
    "        profiles_data = {\n",
    "            \"headers\": all_headers,\n",
    "            \"drivers\": normalized_profiles\n",
    "        }\n",
    "        \n",
    "        profiles_file = os.path.join(DATA_DIR, f\"{current_year}_driver_profiles.json\")\n",
    "        with open(profiles_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(profiles_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"Saved {len(driver_profiles)} driver profiles to {profiles_file}\")\n",
    "        \n",
    "        return all_headers, normalized_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_f1_driver_data(all_driver_links):\n",
    "    \"\"\"Scrape all F1 driver data organized by year\"\"\"\n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    \n",
    "    # Create a longer timeout\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    # Create checkpoint directory and main data directory\n",
    "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Group driver links by year\n",
    "    driver_links_by_year = {}\n",
    "    for name, url, year in all_driver_links:\n",
    "        if year not in driver_links_by_year:\n",
    "            driver_links_by_year[year] = []\n",
    "        driver_links_by_year[year].append((name, url))\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Process driver standings checkpoints\n",
    "        print(\"Processing driver standings...\")\n",
    "        standings_results = []\n",
    "        checkpoint_count = 0\n",
    "        \n",
    "        # Process each year\n",
    "        for year, year_links in driver_links_by_year.items():\n",
    "            # Create directory for the year\n",
    "            year_dir = os.path.join(DATA_DIR, str(year))\n",
    "            os.makedirs(year_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"Processing {len(year_links)} drivers for year {year}\")\n",
    "            \n",
    "            # Process driver results\n",
    "            driver_results = []\n",
    "            results_processed = 0\n",
    "            \n",
    "            for i, link in enumerate(year_links):\n",
    "                driver_name, url = link\n",
    "                \n",
    "                # Process the driver data\n",
    "                result = await process_driver_data(session, link)\n",
    "                \n",
    "                if result:\n",
    "                    driver_results.append(result)\n",
    "                    \n",
    "                    # Save directly to hierarchical structure\n",
    "                    driver_name = result['name'].lower().replace(' ', '_')\n",
    "                    driver_file = os.path.join(year_dir, f\"{driver_name}.json\")\n",
    "                    \n",
    "                    with open(driver_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "                        \n",
    "                    results_processed += 1\n",
    "                \n",
    "                # Save checkpoint every 100 drivers or at the end\n",
    "                checkpoint_file = os.path.join(CHECKPOINTS_DIR, \"driver_results_latest.json\")\n",
    "                if (i + 1) % 100 == 0 or i == len(year_links) - 1:\n",
    "                    checkpoint_count += 1\n",
    "                    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(driver_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"Processed {results_processed} drivers for year {year}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # # Create a summary file\n",
    "    # summary = {\n",
    "    #     \"total_standing_entries\": sum(len(links) for links in driver_links_by_year.values()),\n",
    "    #     \"total_driver_files\": sum(item[\"processed_count\"] for item in standings_results),\n",
    "    #     \"years_processed\": len(driver_links_by_year),\n",
    "    #     \"execution_time\": total_time\n",
    "    # }\n",
    "    \n",
    "    # with open(os.path.join(DATA_DIR, \"summary.json\"), 'w') as f:\n",
    "    #     json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"driver_standings\": standings_results,\n",
    "        \"execution_time\": total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1656 driver standings to race_standing.json\n",
      "Found 21 drivers for the 2025 season\n",
      "Collected profile for Oscar Piastri\n",
      "Collected profile for Lando Norris\n",
      "Collected profile for Max Verstappen\n",
      "Collected profile for George Russell\n",
      "Collected profile for Charles Leclerc\n",
      "Collected profile for Lewis Hamilton\n",
      "Collected profile for Kimi Antonelli\n",
      "Collected profile for Alexander Albon\n",
      "Collected profile for Isack Hadjar\n",
      "Collected profile for Esteban Ocon\n",
      "Collected profile for Nico Hulkenberg\n",
      "Collected profile for Lance Stroll\n",
      "Collected profile for Carlos Sainz\n",
      "Collected profile for Pierre Gasly\n",
      "Collected profile for Yuki Tsunoda\n",
      "Collected profile for Oliver Bearman\n",
      "Collected profile for Liam Lawson\n",
      "Collected profile for Fernando Alonso\n",
      "Collected profile for Gabriel Bortoleto\n",
      "Collected profile for Jack Doohan\n",
      "Collected profile for Franco Colapinto\n",
      "Saved 21 driver profiles to c:\\Users\\anhvi\\OneDrive\\Desktop\\F1 Projekt\\data\\f1_drivers_data\\2025_driver_profiles.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # First collect all driver links\n",
    "    collect_links = await collect_driver_links()\n",
    "\n",
    "    # Collect detailed profiles for current season drivers\n",
    "    await collect_current_driver_profiles()\n",
    "\n",
    "    # Then process all drivers with the collected links\n",
    "    all_data = await scrape_f1_driver_data(collect_links[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
