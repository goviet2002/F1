{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3389fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anhvi\\OneDrive\\Desktop\\F1 Projekt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "from utils.path import get_project_root\n",
    "from utils.f1_shared import ssl_context, head, base_url, years, test_function\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"f1_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "CHECKPOINTS_DIR = os.path.join(PROJECT_ROOT, \"data\", \"f1_checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a0de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_races_year(session, year):\n",
    "    # URL of the page\n",
    "    url = f\"{base_url}/en/results/{year}/races\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    async with session.get(url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {url}. Status: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Find table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        \n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            \n",
    "            rows = table.find('tbody').find_all('tr')\n",
    "            data = []\n",
    "            race_links = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                row_data = []\n",
    "                \n",
    "                for i, col in enumerate(cols):\n",
    "                    if i == 2: #Driver column\n",
    "                        winner = col.text.strip().replace(\"\\xa0\", \" \")[:-3]\n",
    "                        row_data.append(winner)\n",
    "                    else:\n",
    "                        row_data.append(col.text.strip())\n",
    "                        \n",
    "                # Append the row data to the data list (only once per row)\n",
    "                data.append(row_data)\n",
    "                \n",
    "                # Extract race link\n",
    "                race_link = cols[0].find('a')['href']\n",
    "                full_link = f\"{base_url}/en/results/{year}/{race_link}\"\n",
    "                race_links.append((row_data[0], full_link))\n",
    "                \n",
    "        return data, headers, race_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c7b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_race_location(session, race_url):\n",
    "    async with session.get(race_url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {race_url}. Status: {response.status_code}\")\n",
    "            return []\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Find the location table\n",
    "        header_section = soup.find('div', class_='max-tablet:flex-col flex gap-xs')\n",
    "        \n",
    "        if header_section:\n",
    "            location_info = header_section.find_all('p')\n",
    "            \n",
    "            race_date = location_info[0].text.strip()\n",
    "            track = location_info[1].text.strip().split(\", \")\n",
    "            circuit = track[0]\n",
    "            city = track[1]\n",
    "            \n",
    "        return race_date, circuit, city\n",
    "    \n",
    "async def process_race_location(session, race_link_tuple):\n",
    "    grand_prix, url = race_link_tuple\n",
    "    year = url.split('/results/')[1].split('/')[0]\n",
    "\n",
    "    try:\n",
    "        result = await scrape_race_location(session, url)\n",
    "        race_date, circuit, city = result\n",
    "        return [grand_prix, circuit, city, year, race_date]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d806477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available sessions for a race\n",
    "async def scrape_race_sessions(session, race_url):\n",
    "    async with session.get(race_url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {race_url}. Status: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        html = await response.text()   \n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        session_items = soup.find_all(\"ul\", class_=\"f1-sidebar-wrapper\")\n",
    "        li = session_items[0].find_all(\"li\")\n",
    "        sessions = []\n",
    "        \n",
    "        for item in li:\n",
    "                link = item.find(\"a\")\n",
    "                if link:\n",
    "                    session_name = link.text.strip()\n",
    "                    session_link = f\"{base_url}{link['href']}\"\n",
    "                    sessions.append((session_name, session_link))\n",
    "        if not sessions:\n",
    "            None\n",
    "\n",
    "        return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afded3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_race_results(session, session_url, session_name=None):\n",
    "    async with session.get(session_url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {session_url}. Status: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "        # Find the location table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        \n",
    "        if not table:\n",
    "            print(f\"No table found for {session_url}\")\n",
    "            return None\n",
    "        \n",
    "        headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            row_data = []\n",
    "            \n",
    "            for i, col in enumerate(cols):\n",
    "                if i == 2: #Driver column\n",
    "                    winner = col.text.strip().replace(\"\\xa0\", \" \")[:-3]\n",
    "                    row_data.append(winner)\n",
    "                else:\n",
    "                    row_data.append(col.text.strip())\n",
    "                    \n",
    "            data.append(row_data)\n",
    "        return headers, data, session_url, session_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_race_location = ['Grand Prix', 'Circuit', 'Country/City', 'Year', 'Date']\n",
    "race_location = []\n",
    "\n",
    "# Collect all race links\n",
    "async def collect_race_links():\n",
    "    all_race_links = []\n",
    "    headers_race = []\n",
    "    races = []\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector) as session:          \n",
    "        tasks = [scrape_races_year(session, year) for year in years]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "        for race, header_race, race_links in results:\n",
    "            races.extend(race)\n",
    "            all_race_links.extend([(link[0], link[1]) for link in race_links])\n",
    "\n",
    "            if len(headers_race) == 0:\n",
    "                headers_race = header_race\n",
    "                \n",
    "        # Save the races data to a JSON file\n",
    "        races_data = {\n",
    "            \"headers\": headers_race,\n",
    "            \"races\": races\n",
    "        }    \n",
    "        with open(os.path.join(DATA_DIR, \"races.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(races_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved {len(races)} races to all_races.json\")\n",
    "                \n",
    "        return all_race_links, headers_race, races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edfd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all race links \n",
    "collect_links = await collect_race_links()\n",
    "print(collect_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_f1_data_with_checkpoints(all_race_links):\n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    \n",
    "    # Create a longer timeout\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Process Race Location concurrently with incremental saves\n",
    "        print(\"Processing race locations...\")\n",
    "        location_results = []\n",
    "        checkpoint_count = 0\n",
    "        \n",
    "        for i, link in enumerate(all_race_links):\n",
    "            result = await process_race_location(session, link)\n",
    "            \n",
    "            if result:  # Only process valid results\n",
    "                location_results.append(result)\n",
    "                \n",
    "                # Save directly to hierarchical structure\n",
    "                grand_prix, circuit, city, year, date = result\n",
    "                \n",
    "                # Create safe directory path \n",
    "                gp_name = grand_prix.lower().replace(' ', '_')\n",
    "                race_dir = os.path.join(DATA_DIR, str(year), gp_name)\n",
    "                os.makedirs(race_dir, exist_ok=True)\n",
    "                \n",
    "                # Save race metadata\n",
    "                metadata = {\n",
    "                    \"grand_prix\": grand_prix,\n",
    "                    \"circuit\": circuit,\n",
    "                    \"city\": city, \n",
    "                    \"year\": year,\n",
    "                    \"date\": date\n",
    "                }\n",
    "                \n",
    "                with open(os.path.join(race_dir, \"race_metadata.json\"), 'w', encoding='utf-8') as f:\n",
    "                    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Save checkpoint every 100 races or at the end\n",
    "            checkpoint_file =  os.path.join(CHECKPOINTS_DIR, \"race_locations_latest.json\")\n",
    "            if (i + 1) % 1000 == 0 or i == len(all_race_links) - 1:\n",
    "                checkpoint_count += 1\n",
    "                with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(location_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Processed {len(location_results)} race locations\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # Process Race Sessions with incremental saves\n",
    "        print(\"Getting race sessions...\")\n",
    "        session_results = []\n",
    "        all_sessions = []\n",
    "        checkpoint_count = 0\n",
    "        \n",
    "        for i, link in enumerate(all_race_links):\n",
    "            sessions = await scrape_race_sessions(session, link[1])\n",
    "            \n",
    "            if sessions:\n",
    "                session_results.append(sessions)\n",
    "                all_sessions.extend(sessions)\n",
    "                \n",
    "            # Save checkpoint every 100 races or at the end\n",
    "            checkpoint_file =  os.path.join(CHECKPOINTS_DIR,\"race_sessions_latest.json\")\n",
    "            if (i + 1) % 1000 == 0 or i == len(all_race_links) - 1:\n",
    "                checkpoint_count += 1\n",
    "                with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(session_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Found {len(all_sessions)} total session results to process\")\n",
    "\n",
    "        # Scrape Race Results with incremental saves to hierarchical structure\n",
    "        print(\"Processing race results...\")\n",
    "        race_result = {}\n",
    "        checkpoint_count = 0\n",
    "        results_processed = 0\n",
    "        \n",
    "        for i, task in enumerate(all_sessions):\n",
    "            result = await scrape_race_results(session, task[1], task[0])\n",
    "            \n",
    "            if result is not None:\n",
    "                headers, data, url, session_name = result\n",
    "                race_result[url] = {\n",
    "                    \"header\": headers,\n",
    "                    \"data\": data,\n",
    "                    \"session_name\": session_name\n",
    "                }\n",
    "                \n",
    "                # Save directly to hierarchical structure\n",
    "                parts = url.split('/')\n",
    "                year = parts[5]\n",
    "                \n",
    "                # Extract race location from URL (bahrain, etc)\n",
    "                race_location = parts[8] if len(parts) > 8 else \"unknown\"\n",
    "                race_location = race_location.replace('-', '_')\n",
    "                session_type = session_name.lower().replace(' ', '-').replace('-', '_')\n",
    "                \n",
    "                # Create directory path\n",
    "                race_dir = os.path.join(DATA_DIR, str(year), race_location)\n",
    "                os.makedirs(race_dir, exist_ok=True)\n",
    "                \n",
    "                # Save session data\n",
    "                session_filename = f\"{session_type}.json\"\n",
    "                with open(os.path.join(race_dir, session_filename), 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\n",
    "                        \"header\": headers,\n",
    "                        \"data\": data,\n",
    "                        \"session_name\": session_name\n",
    "                    }, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                results_processed += 1\n",
    "                \n",
    "            # Save checkpoint every 200 sessions or at the end\n",
    "            checkpoint_file =  os.path.join(CHECKPOINTS_DIR, \"race_results_latest.json\")\n",
    "            if (i + 1) % 1000 == 0 or i == len(all_sessions) - 1:\n",
    "                checkpoint_count += 1\n",
    "                with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(race_result, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Processed {results_processed} race results\")\n",
    "        print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "        \n",
    "        # Delete checkpoint files after successful completion\n",
    "        checkpoint_files = [\n",
    "            os.path.join(CHECKPOINTS_DIR, \"race_locations_latest.json\"),\n",
    "            os.path.join(CHECKPOINTS_DIR, \"race_sessions_latest.json\"),\n",
    "            os.path.join(CHECKPOINTS_DIR, \"race_results_latest.json\")\n",
    "        ]\n",
    "        \n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            if os.path.exists(checkpoint_file):\n",
    "                os.remove(checkpoint_file)\n",
    "                print(f\"Deleted checkpoint file: {checkpoint_file}\")\n",
    "\n",
    "        # # Create a summary file\n",
    "        # summary = {\n",
    "        #     \"total_races\": len(location_results),\n",
    "        #     \"total_sessions\": len(all_sessions),\n",
    "        #     \"total_results\": results_processed,\n",
    "        #     \"execution_time\": total_time\n",
    "        # }\n",
    "        \n",
    "        # with open(os.path.join(DATA_DIR, \"summary.json\"), 'w') as f:\n",
    "        #     json.dump(summary, f, indent=2)\n",
    "\n",
    "        # Return the results\n",
    "        return {\n",
    "            \"race_location\": location_results,\n",
    "            \"race_sessions\": all_sessions,\n",
    "            \"race_result\": race_result,\n",
    "            \"execution_time\": total_time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = await scrape_f1_data_with_checkpoints(collect_links[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
