{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53024826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "from utils.path import get_project_root\n",
    "from utils.f1_shared import ssl_context, head, base_url, years, test_function\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"f1_teams_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "CHECKPOINTS_DIR = os.path.join(PROJECT_ROOT, \"data\", \"f1_checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c3518c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_teams_standing(session, year):\n",
    "    \"\"\"Scrape team standings for a specific year\"\"\"\n",
    "    url = f\"{base_url}/en/results/{year}/team\"\n",
    "    \n",
    "    # Initialize variables with default empty values\n",
    "    data = []\n",
    "    headers = []\n",
    "    team_links = []\n",
    "    \n",
    "    async with session.get(url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {url}. Status: {response.status}\")\n",
    "            return data, headers, team_links\n",
    "\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Find table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        \n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            # Add YEAR to headers\n",
    "            if 'Year' not in headers:\n",
    "                headers.append('Year')\n",
    "            \n",
    "            rows = table.find('tbody').find_all('tr')\n",
    "            data = []\n",
    "            team_links = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                row_data = []\n",
    "                \n",
    "                for i, col in enumerate(cols):\n",
    "                    if i == 1:  # Team name column\n",
    "                        # Extract team name\n",
    "                        team_a = col.find('a')\n",
    "                        if team_a:\n",
    "                            team_name = team_a.text.strip()\n",
    "                            row_data.append(team_name)\n",
    "                        else:\n",
    "                            row_data.append(col.text.strip())\n",
    "                    else:\n",
    "                        row_data.append(col.text.strip())\n",
    "                \n",
    "                # Add year to each row\n",
    "                row_data.append(str(year))\n",
    "                data.append(row_data)\n",
    "                \n",
    "                # Extract team link\n",
    "                team_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "                if team_link:\n",
    "                    # Make sure the team_link starts with a slash if needed\n",
    "                    if not team_link.startswith('/'):\n",
    "                        team_link = f\"/{team_link}\"\n",
    "                        \n",
    "                    # Add the full URL\n",
    "                    full_link = f\"{base_url}/en/results/{year}{team_link}\"\n",
    "                    team_links.append((row_data[1], full_link, year))  # Add year to link tuple\n",
    "                \n",
    "        return data, headers, team_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc09a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pos                          Team  Pts  Year\n",
      "0   1              McLaren Mercedes  666  2024\n",
      "1   2                       Ferrari  652  2024\n",
      "2   3    Red Bull Racing Honda RBPT  589  2024\n",
      "3   4                      Mercedes  468  2024\n",
      "4   5  Aston Martin Aramco Mercedes   94  2024\n",
      "5   6                Alpine Renault   65  2024\n",
      "6   7                  Haas Ferrari   58  2024\n",
      "7   8                 RB Honda RBPT   46  2024\n",
      "8   9             Williams Mercedes   17  2024\n",
      "9  10           Kick Sauber Ferrari    4  2024\n",
      "[('McLaren Mercedes', 'https://www.formula1.com/en/results/2024/team/McLaren-Mercedes', 2024), ('Ferrari', 'https://www.formula1.com/en/results/2024/team/Ferrari', 2024), ('Red Bull Racing Honda RBPT', 'https://www.formula1.com/en/results/2024/team/Red-Bull-Racing-Honda-RBPT', 2024), ('Mercedes', 'https://www.formula1.com/en/results/2024/team/Mercedes', 2024), ('Aston Martin Aramco Mercedes', 'https://www.formula1.com/en/results/2024/team/Aston-Martin-Aramco-Mercedes', 2024), ('Alpine Renault', 'https://www.formula1.com/en/results/2024/team/Alpine-Renault', 2024), ('Haas Ferrari', 'https://www.formula1.com/en/results/2024/team/Haas-Ferrari', 2024), ('RB Honda RBPT', 'https://www.formula1.com/en/results/2024/team/RB-Honda-RBPT', 2024), ('Williams Mercedes', 'https://www.formula1.com/en/results/2024/team/Williams-Mercedes', 2024), ('Kick Sauber Ferrari', 'https://www.formula1.com/en/results/2024/team/Kick-Sauber-Ferrari', 2024)]\n"
     ]
    }
   ],
   "source": [
    "result = await test_function(2024, scrape_teams_standing)\n",
    "df = pd.DataFrame(result[0], columns=result[1])\n",
    "print(df)\n",
    "print(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb587798",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_team_results(session, team_url):\n",
    "    \"\"\"Scrape detailed information for a specific team\"\"\"\n",
    "    async with session.get(team_url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {team_url}. Status: {response.status}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Extract team code from URL\n",
    "        url_parts = team_url.split('/')\n",
    "        team_code = url_parts[-1] if len(url_parts) > 2 else None\n",
    "        \n",
    "        # Get the race results table\n",
    "        table = soup.find('table', class_='f1-table-with-data')\n",
    "        if not table:\n",
    "            print(f\"No results table found for {team_url}\")\n",
    "            return [], [], team_code\n",
    "            \n",
    "        # Get headers\n",
    "        headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "        \n",
    "        # Get race results\n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            row_data = []\n",
    "            \n",
    "            for col in cols:\n",
    "                row_data.append(col.text.strip())\n",
    "                \n",
    "            data.append(row_data)\n",
    "                \n",
    "        return data, headers, team_code\n",
    "    \n",
    "async def process_team_data(session, team_link_tuple):\n",
    "    \"\"\"Process a team link to get detailed information\"\"\"\n",
    "    team_name, url = team_link_tuple\n",
    "\n",
    "    try:\n",
    "        data, headers, team_code = await scrape_team_results(session, url)\n",
    "        \n",
    "        # Create a team details dictionary with all the data\n",
    "        team_details = {\n",
    "            'name': team_name,\n",
    "            'team_code': team_code,\n",
    "            'url': url,\n",
    "            'headers': headers,\n",
    "            'race_results': data\n",
    "        }\n",
    "        \n",
    "        return team_details\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing team {team_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1da393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Grand prix         Date Pts\n",
      "0          Bahrain  02 Mar 2024  12\n",
      "1     Saudi Arabia  09 Mar 2024  16\n",
      "2        Australia  24 Mar 2024  27\n",
      "3            Japan  07 Apr 2024  14\n",
      "4            China  21 Apr 2024  27\n",
      "5            Miami  05 May 2024  28\n",
      "6   Emilia-Romagna  19 May 2024  30\n",
      "7           Monaco  26 May 2024  30\n",
      "8           Canada  09 Jun 2024  28\n",
      "9            Spain  23 Jun 2024  25\n",
      "10         Austria  30 Jun 2024  31\n",
      "11   Great Britain  07 Jul 2024  27\n",
      "12         Hungary  21 Jul 2024  43\n",
      "13         Belgium  28 Jul 2024  28\n",
      "14     Netherlands  25 Aug 2024  38\n",
      "15           Italy  01 Sep 2024  34\n",
      "16      Azerbaijan  15 Sep 2024  38\n",
      "17       Singapore  22 Sep 2024  40\n",
      "18   United States  20 Oct 2024  28\n",
      "19          Mexico  27 Oct 2024  22\n",
      "20          Brazil  03 Nov 2024  27\n",
      "21       Las Vegas  23 Nov 2024  15\n",
      "22           Qatar  01 Dec 2024  32\n",
      "23       Abu Dhabi  08 Dec 2024  26\n"
     ]
    }
   ],
   "source": [
    "result = await test_function('https://www.formula1.com/en/results/2024/team/McLaren-Mercedes', scrape_team_results)\n",
    "df = pd.DataFrame(result[0], columns=result[1])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc36a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_team_links():\n",
    "    \"\"\"Collect all team links across years\"\"\"\n",
    "    all_team_links = []\n",
    "    headers_teams = []\n",
    "    teams = []\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector) as session:          \n",
    "        tasks = [scrape_teams_standing(session, year) for year in years]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "        for team_data, header_team, team_links in results:\n",
    "            teams.extend(team_data)\n",
    "            all_team_links.extend([(link[0], link[1], link[2]) for link in team_links])\n",
    "\n",
    "            if len(headers_teams) == 0:\n",
    "                headers_teams = header_team\n",
    "                \n",
    "        # Save the teams data to a JSON file\n",
    "        teams_data = {\n",
    "            \"headers\": headers_teams,\n",
    "            \"teams\": teams\n",
    "        }    \n",
    "        with open(os.path.join(DATA_DIR, \"team_standing.json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(teams_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved {len(teams)} team standings to team_standing.json\")\n",
    "                \n",
    "        return all_team_links, headers_teams, teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83504696",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_team_profile(session, team_name, team_code):\n",
    "    \"\"\"Scrape detailed profile information for a team from the main teams page\"\"\"\n",
    "    # Extract the team name part for the URL\n",
    "    name_part = team_name.lower().replace(' ', '-')\n",
    "    \n",
    "    # Construct profile URL\n",
    "    profile_url = f\"{base_url}/en/teams/{name_part}.html\"\n",
    "    \n",
    "    try:\n",
    "        async with session.get(profile_url, headers=head) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Team profile not found: {profile_url}. Status: {response.status}\")\n",
    "                return None, None\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "            # Get the team info section (with Full Team Name, Base, etc.)\n",
    "            team_info_section = soup.find('div', class_='f1-dl')\n",
    "            \n",
    "            # Initialize lists for headers and data\n",
    "            headers = [\"name\", \"team_code\", \"profile_url\"]\n",
    "            data = [team_name, team_code, profile_url]\n",
    "            \n",
    "            if team_info_section:\n",
    "                # Extract info from dt/dd pairs\n",
    "                dt_elements = team_info_section.find_all('dt')\n",
    "                dd_elements = team_info_section.find_all('dd')\n",
    "                \n",
    "                for dt, dd in zip(dt_elements, dd_elements):\n",
    "                    # Convert header to lowercase with underscores\n",
    "                    header = dt.text.strip().lower().replace(' ', '_')\n",
    "                    data.append(dd.text.strip())\n",
    "                    headers.append(header)\n",
    "\n",
    "            # Get driver information\n",
    "            drivers_section = soup.select('figure.bg-brand-white')\n",
    "            \n",
    "            if drivers_section and len(drivers_section) >= 1:\n",
    "                # First driver\n",
    "                driver1_div = drivers_section[0].find('figcaption').find('div')\n",
    "                if driver1_div:\n",
    "                    # Get driver number\n",
    "                    driver1_number_elem = driver1_div.find('p', class_='f1-heading')\n",
    "                    # Get driver name\n",
    "                    driver1_name_elem = driver1_div.find_all('p', class_='f1-heading')[1] if len(driver1_div.find_all('p', class_='f1-heading')) > 1 else None\n",
    "                    \n",
    "                    if driver1_number_elem:\n",
    "                        headers.append(\"driver_1_no\")\n",
    "                        data.append(driver1_number_elem.text.strip())\n",
    "                    if driver1_name_elem:\n",
    "                        headers.append(\"driver_1\")\n",
    "                        data.append(driver1_name_elem.text.strip())\n",
    "            \n",
    "            if drivers_section and len(drivers_section) >= 2:\n",
    "                # Second driver\n",
    "                driver2_div = drivers_section[1].find('figcaption').find('div')\n",
    "                if driver2_div:\n",
    "                    # Get driver number\n",
    "                    driver2_number_elem = driver2_div.find('p', class_='f1-heading')\n",
    "                    # Get driver name\n",
    "                    driver2_name_elem = driver2_div.find_all('p', class_='f1-heading')[1] if len(driver2_div.find_all('p', class_='f1-heading')) > 1 else None\n",
    "                    \n",
    "                    if driver2_number_elem:\n",
    "                        headers.append(\"driver_2_no\")\n",
    "                        data.append(driver2_number_elem.text.strip())\n",
    "                    if driver2_name_elem:\n",
    "                        headers.append(\"driver_2\")\n",
    "                        data.append(driver2_name_elem.text.strip())\n",
    "            \n",
    "            return headers, data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping profile for {team_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "async def scrape_teams_listing(session):\n",
    "    \"\"\"Scrape teams directly from the main F1 teams listing page\"\"\"\n",
    "    url = f\"{base_url}/en/teams\"\n",
    "    \n",
    "    async with session.get(url, headers=head) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load {url}. Status: {response.status}\")\n",
    "            return []\n",
    "\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Find all team links\n",
    "        team_items = soup.select('a[href^=\"/en/teams/\"]')\n",
    "        team_links = []\n",
    "        \n",
    "        for item in team_items:\n",
    "            team_url = item['href']\n",
    "            if team_url.count('/') >= 3:  # Make sure it's a team details page\n",
    "                # Extract team name\n",
    "                team_name_elem = item.select_one('.f1-heading[class*=\"font-bold\"]')\n",
    "                if team_name_elem:\n",
    "                    team_name = team_name_elem.text.strip()\n",
    "                    \n",
    "                    # Extract position\n",
    "                    position_elem = item.select_one('.f1-heading-black')\n",
    "                    position = position_elem.text.strip() if position_elem else \"\"\n",
    "                    \n",
    "                    # Extract points\n",
    "                    points_elem = item.select_one('.f1-heading-wide')\n",
    "                    points = points_elem.text.strip() if points_elem else \"\"\n",
    "                    \n",
    "                    # Extract team logo URL\n",
    "                    logo_elem = item.select_one('img[alt=\"' + team_name + '\"]')\n",
    "                    logo_url = logo_elem['src'] if logo_elem else \"\"\n",
    "                    \n",
    "                    # Extract team color - find the parent div that has text-COLOR class\n",
    "                    team_card = item.select_one('div[class*=\"text-\"]')\n",
    "                    team_color = \"#\"\n",
    "                    if team_card:\n",
    "                        color_classes = [c for c in team_card.get('class', []) if c.startswith('text-') and not c == 'text-brand-black']\n",
    "                        if color_classes:\n",
    "                            team_color += color_classes[0].replace('text-', '')\n",
    "                    \n",
    "                    # Extract car image URL\n",
    "                    car_img = item.select_one('.flex.items-baseline img')\n",
    "                    car_img_url = car_img['src'] if car_img else \"\"\n",
    "                    \n",
    "                    # Get team code from URL\n",
    "                    team_code = team_url.split('/')[-1]\n",
    "                    \n",
    "                    team_data = {\n",
    "                        'name': team_name,\n",
    "                        'team_code': team_code,\n",
    "                        'position': position,\n",
    "                        'points': points,\n",
    "                        'logo_url': logo_url,\n",
    "                        'car_img_url': car_img_url, \n",
    "                        'team_color': team_color,\n",
    "                        'year': years[-1]  # Current year\n",
    "                    }\n",
    "                    \n",
    "                    team_links.append(team_data)\n",
    "        \n",
    "        return team_links\n",
    "\n",
    "async def collect_current_teams_data():\n",
    "    \"\"\"Collect comprehensive team data from the main teams page and individual profiles\"\"\"\n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Get teams from main listing page\n",
    "        teams_basic_data = await scrape_teams_listing(session)\n",
    "        \n",
    "        # Process each team to get detailed profile\n",
    "        all_team_data = []\n",
    "        \n",
    "        for team in teams_basic_data:\n",
    "            team_name = team['name']\n",
    "            team_code = team['team_code']\n",
    "            \n",
    "            # Get detailed profile data\n",
    "            headers, data = await scrape_team_profile(session, team_name, team_code)\n",
    "            \n",
    "            if headers and data:\n",
    "                # Create a profile dictionary\n",
    "                profile_dict = dict(zip(headers, data))\n",
    "                \n",
    "                # Merge with basic data\n",
    "                for key, value in profile_dict.items():\n",
    "                    if key not in team or not team[key]:  # Don't overwrite existing values\n",
    "                        team[key] = value\n",
    "                \n",
    "                all_team_data.append(team)\n",
    "                print(f\"Processed team: {team_name}\")\n",
    "            else:\n",
    "                # Still add the basic team data even if profile fetch failed\n",
    "                all_team_data.append(team)\n",
    "                print(f\"Added basic data for team: {team_name} (profile fetch failed)\")\n",
    "        \n",
    "        # Save the complete team data\n",
    "        current_year = years[-1]\n",
    "        profiles_file = os.path.join(DATA_DIR, f\"{current_year}_team_profiles.json\")\n",
    "        \n",
    "        with open(profiles_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_team_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved complete data for {len(all_team_data)} teams to {profiles_file}\")\n",
    "        \n",
    "        return all_team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35146ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_f1_team_data(all_team_links):\n",
    "    \"\"\"Scrape all F1 team data organized by year\"\"\"\n",
    "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
    "    \n",
    "    # Create a longer timeout\n",
    "    timeout = aiohttp.ClientTimeout(total=60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Group team links by year\n",
    "    team_links_by_year = {}\n",
    "    for name, url, year in all_team_links:\n",
    "        if year not in team_links_by_year:\n",
    "            team_links_by_year[year] = []\n",
    "        team_links_by_year[year].append((name, url))\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Process team standings checkpoints\n",
    "        print(\"Processing team standings...\")\n",
    "        standings_results = []\n",
    "        checkpoint_count = 0\n",
    "        \n",
    "        # Process each year\n",
    "        for year, year_links in team_links_by_year.items():\n",
    "            # Create directory for the year\n",
    "            year_dir = os.path.join(DATA_DIR, str(year))\n",
    "            os.makedirs(year_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"Processing {len(year_links)} teams for year {year}\")\n",
    "            \n",
    "            # Process team results\n",
    "            team_results = []\n",
    "            results_processed = 0\n",
    "            \n",
    "            for i, link in enumerate(year_links):\n",
    "                team_name, url = link\n",
    "                \n",
    "                # Process the team data\n",
    "                result = await process_team_data(session, link)\n",
    "                \n",
    "                if result:\n",
    "                    team_results.append(result)\n",
    "                    \n",
    "                    team_name = result['name'].lower()\n",
    "                    # Sanitize filename by replacing invalid characters\n",
    "                    team_name = team_name.replace('/', '_').replace('\\\\', '_')  # Handle path separators first\n",
    "                    team_name = team_name.replace(' ', '_').replace('?', '').replace('*', '')\n",
    "                    team_name = team_name.replace(':', '').replace('\"', '').replace('<', '').replace('>', '')\n",
    "                    team_file = os.path.join(year_dir, f\"{team_name}.json\")\n",
    "                    \n",
    "                    with open(team_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "                        \n",
    "                    results_processed += 1\n",
    "                \n",
    "                # Save checkpoint every 100 teams or at the end\n",
    "                checkpoint_file = os.path.join(CHECKPOINTS_DIR, \"team_results_latest.json\")\n",
    "                if (i + 1) % 100 == 0 or i == len(year_links) - 1:\n",
    "                    checkpoint_count += 1\n",
    "                    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(team_results, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "            print(f\"Processed {results_processed} teams for year {year}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # Delete checkpoint file after successful completion\n",
    "    checkpoint_file = os.path.join(CHECKPOINTS_DIR, \"team_results_latest.json\")\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "        print(f\"Deleted checkpoint file: {checkpoint_file}\")\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"team_standings\": standings_results,\n",
    "        \"execution_time\": total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ccdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 705 team standings to team_standing.json\n",
      "Processed team: McLaren\n",
      "Processed team: Ferrari\n",
      "Processed team: Mercedes\n",
      "Processed team: Red Bull Racing\n",
      "Processed team: Williams\n",
      "Processed team: Racing Bulls\n",
      "Processed team: Haas\n",
      "Processed team: Kick Sauber\n",
      "Processed team: Aston Martin\n",
      "Processed team: Alpine\n",
      "Saved complete data for 10 teams to c:\\Users\\anhvi\\OneDrive\\Desktop\\F1 Projekt\\data\\f1_teams_data\\2025_team_profiles.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # First collect all team links\n",
    "    collect_links = await collect_team_links()\n",
    "    \n",
    "    # Collect current teams data from the main teams page and detailed profiles\n",
    "    current_teams = await collect_current_teams_data()\n",
    "\n",
    "    # Then process all teams with the collected links\n",
    "    all_data = await scrape_f1_team_data(collect_links[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
